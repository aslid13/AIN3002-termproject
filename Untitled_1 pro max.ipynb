{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST\n",
    "clifften düştüğü görülüyor, test sonuçları beklenilen gibi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jLko_68qOq9R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1LmZ4fxOq9T",
    "outputId": "9a32f45e-4534-4d34-9f7c-cd57cc680568"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() :\n",
    "    device = torch.device(\"mps\")\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkI9iHsxOq9U"
   },
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ec8eufhyOq9W"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ca7c3b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 784\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256 #16\n",
    "epochs = 10\n",
    "torch.manual_seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lo6v4OfOq9W"
   },
   "source": [
    "Initializing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XSr4kPOZOq9X",
    "outputId": "6ab53924-8c34-4799-b5e5-d21f8f03c77c"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"datasets/\",train=True,transform=transforms.ToTensor(),download=True)\n",
    "train_loader = DataLoader(dataset = train_dataset,batch_size=batch_size,shuffle = True)\n",
    "test_dataset = datasets.MNIST(root=\"datasets/\",train=False,transform=transforms.ToTensor(),download=True)\n",
    "test_loader = DataLoader(dataset = test_dataset,batch_size=batch_size,shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lm-Q5nnfOq9X"
   },
   "source": [
    "Network Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing 3 different architectures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wSqAGMAwOq9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got59801/60000 with accuracy 99.67\n",
      "Got9774/10000 with accuracy 97.74\n"
     ]
    }
   ],
   "source": [
    "#architecture 1\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input,classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input,256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "model = NN(input_size,classes = num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device = device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        #setting each gradient to 0 \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #gradient descent\n",
    "        optimizer.step()\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    model.train()\n",
    "check_accuracy(train_loader,model)\n",
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got59875/60000 with accuracy 99.79\n",
      "Got9795/10000 with accuracy 97.95\n"
     ]
    }
   ],
   "source": [
    "#architecture 2\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input,classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "model = NN(input_size,classes = num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device = device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        #setting each gradient to 0 \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #gradient descent\n",
    "        optimizer.step()\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    model.train()\n",
    "check_accuracy(train_loader,model)\n",
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got59406/60000 with accuracy 99.01\n",
      "Got9762/10000 with accuracy 97.62\n"
     ]
    }
   ],
   "source": [
    "#architecture 3\n",
    "class NN(nn.Module):\n",
    "    def __init__(self,input,classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input,128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "model = NN(input_size,classes = num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range (epochs):\n",
    "    for batch_idx, (data,target) in enumerate(train_loader):\n",
    "        data = data.to(device=device)\n",
    "        target = target.to(device = device)\n",
    "        \n",
    "        data = data.reshape(data.shape[0],-1)\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, target)\n",
    "        #setting each gradient to 0 \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        #gradient descent\n",
    "        optimizer.step()\n",
    "def check_accuracy(loader,model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            x = x.reshape(x.shape[0],-1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "    print(f'Got{num_correct}/{num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')\n",
    "    model.train()\n",
    "check_accuracy(train_loader,model)\n",
    "check_accuracy(test_loader,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the best architecture to apply dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout rate: 0.2\n",
      "Training set:\n",
      "Got59828/60000 with accuracy 99.71\n",
      "Test set:\n",
      "Got9816/10000 with accuracy 98.16\n",
      "\n",
      "Training with dropout rate: 0.3\n",
      "Training set:\n",
      "Got59783/60000 with accuracy 99.64\n",
      "Test set:\n",
      "Got9823/10000 with accuracy 98.23\n",
      "\n",
      "Training with dropout rate: 0.4\n",
      "Training set:\n",
      "Got59734/60000 with accuracy 99.56\n",
      "Test set:\n",
      "Got9835/10000 with accuracy 98.35\n",
      "\n",
      "Training with dropout rate: 0.5\n",
      "Training set:\n",
      "Got59577/60000 with accuracy 99.30\n",
      "Test set:\n",
      "Got9799/10000 with accuracy 97.99\n",
      "\n",
      "Training with dropout rate: 0.6\n",
      "Training set:\n",
      "Got59387/60000 with accuracy 98.98\n",
      "Test set:\n",
      "Got9789/10000 with accuracy 97.89\n",
      "\n",
      "Training with dropout rate: 0.7\n",
      "Training set:\n",
      "Got59071/60000 with accuracy 98.45\n",
      "Test set:\n",
      "Got9783/10000 with accuracy 97.83\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#architecture 2 with dropout\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]  # List of dropout rates to try\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout_rate):\n",
    "        super(NN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"Training with dropout rate: {dropout_rate}\")\n",
    "    model = NN(input_size, num_classes, dropout_rate).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device=device)\n",
    "            target = target.to(device=device)\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "            \n",
    "            # Forward pass\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluate on training and test sets\n",
    "    print(\"Training set:\")\n",
    "    check_accuracy(train_loader, model)\n",
    "    print(\"Test set:\")\n",
    "    check_accuracy(test_loader, model)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jinC2KEaUPA5"
   },
   "source": [
    "Cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "q6Dz9gk287PG"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(20)\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "n5BGGzG9UQQA"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLJGsSs1ajRA",
    "outputId": "becb8210-e591-49e5-b84a-a207d660898c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root=\"datasets/\", train = True, download = True, transform = transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_dataset = datasets.CIFAR10(root=\"datasets/\",train=False, download = True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 44.41 %\n",
      "Accuracy of plane: 47.4 %\n",
      "Accuracy of car: 56.8 %\n",
      "Accuracy of bird: 32.5 %\n",
      "Accuracy of cat: 24.8 %\n",
      "Accuracy of deer: 32.4 %\n",
      "Accuracy of dog: 34.8 %\n",
      "Accuracy of frog: 59.0 %\n",
      "Accuracy of horse: 54.1 %\n",
      "Accuracy of ship: 51.6 %\n",
      "Accuracy of truck: 50.7 %\n"
     ]
    }
   ],
   "source": [
    "#architecture 1     \n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "  for batch_idx, (images,labels) in enumerate (train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for _ in range(10)]\n",
    "    n_class_samples = [0 for _ in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 45.64 %\n",
      "Accuracy of plane: 51.9 %\n",
      "Accuracy of car: 65.9 %\n",
      "Accuracy of bird: 29.1 %\n",
      "Accuracy of cat: 27.7 %\n",
      "Accuracy of deer: 28.7 %\n",
      "Accuracy of dog: 34.2 %\n",
      "Accuracy of frog: 58.5 %\n",
      "Accuracy of horse: 52.5 %\n",
      "Accuracy of ship: 55.5 %\n",
      "Accuracy of truck: 52.4 %\n"
     ]
    }
   ],
   "source": [
    "#architecture 2 \n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "  for batch_idx, (images,labels) in enumerate (train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for _ in range(10)]\n",
    "    n_class_samples = [0 for _ in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 47.42 %\n",
      "Accuracy of plane: 46.4 %\n",
      "Accuracy of car: 48.5 %\n",
      "Accuracy of bird: 22.4 %\n",
      "Accuracy of cat: 22.5 %\n",
      "Accuracy of deer: 31.2 %\n",
      "Accuracy of dog: 51.3 %\n",
      "Accuracy of frog: 72.2 %\n",
      "Accuracy of horse: 55.0 %\n",
      "Accuracy of ship: 58.5 %\n",
      "Accuracy of truck: 66.2 %\n"
     ]
    }
   ],
   "source": [
    "#architecture 3 \n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "  for batch_idx, (images,labels) in enumerate (train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for _ in range(10)]\n",
    "    n_class_samples = [0 for _ in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 63.48 %\n",
      "Accuracy of plane: 77.7 %\n",
      "Accuracy of car: 78.3 %\n",
      "Accuracy of bird: 49.9 %\n",
      "Accuracy of cat: 42.5 %\n",
      "Accuracy of deer: 54.9 %\n",
      "Accuracy of dog: 42.5 %\n",
      "Accuracy of frog: 80.7 %\n",
      "Accuracy of horse: 66.3 %\n",
      "Accuracy of ship: 75.1 %\n",
      "Accuracy of truck: 66.9 %\n"
     ]
    }
   ],
   "source": [
    "#architecture 3 with momentum\n",
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "total_steps = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "  for batch_idx, (images,labels) in enumerate (train_loader):\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      loss = criterion(outputs, labels)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for _ in range(10)]\n",
    "    n_class_samples = [0 for _ in range(10)]\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            \n",
    "            n_class_samples[label] += 1\n",
    "            if label == pred:\n",
    "                n_class_correct[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size düşük olduğu için çok uzun sürüyor (20 dk sürdü collabde ne kadar sürer bilemem), yüksek batch sizelarda epoch fark etmeksizin accuracy tek haneye düştüğü için bu böyle kalacak sanırım\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network with dropout rate 0.2: 64.43 %\n",
      "Accuracy of plane with dropout rate 0.2: 72.7 %\n",
      "Accuracy of car with dropout rate 0.2: 74.5 %\n",
      "Accuracy of bird with dropout rate 0.2: 60.3 %\n",
      "Accuracy of cat with dropout rate 0.2: 47.9 %\n",
      "Accuracy of deer with dropout rate 0.2: 56.0 %\n",
      "Accuracy of dog with dropout rate 0.2: 47.1 %\n",
      "Accuracy of frog with dropout rate 0.2: 78.8 %\n",
      "Accuracy of horse with dropout rate 0.2: 65.4 %\n",
      "Accuracy of ship with dropout rate 0.2: 74.5 %\n",
      "Accuracy of truck with dropout rate 0.2: 67.1 %\n",
      "Accuracy of the network with dropout rate 0.3: 65.5 %\n",
      "Accuracy of plane with dropout rate 0.3: 71.2 %\n",
      "Accuracy of car with dropout rate 0.3: 75.9 %\n",
      "Accuracy of bird with dropout rate 0.3: 51.2 %\n",
      "Accuracy of cat with dropout rate 0.3: 48.6 %\n",
      "Accuracy of deer with dropout rate 0.3: 63.2 %\n",
      "Accuracy of dog with dropout rate 0.3: 40.6 %\n",
      "Accuracy of frog with dropout rate 0.3: 76.7 %\n",
      "Accuracy of horse with dropout rate 0.3: 71.9 %\n",
      "Accuracy of ship with dropout rate 0.3: 83.6 %\n",
      "Accuracy of truck with dropout rate 0.3: 72.1 %\n",
      "Accuracy of the network with dropout rate 0.4: 63.63 %\n",
      "Accuracy of plane with dropout rate 0.4: 73.1 %\n",
      "Accuracy of car with dropout rate 0.4: 72.9 %\n",
      "Accuracy of bird with dropout rate 0.4: 58.2 %\n",
      "Accuracy of cat with dropout rate 0.4: 37.8 %\n",
      "Accuracy of deer with dropout rate 0.4: 52.8 %\n",
      "Accuracy of dog with dropout rate 0.4: 55.5 %\n",
      "Accuracy of frog with dropout rate 0.4: 74.2 %\n",
      "Accuracy of horse with dropout rate 0.4: 71.1 %\n",
      "Accuracy of ship with dropout rate 0.4: 75.5 %\n",
      "Accuracy of truck with dropout rate 0.4: 65.2 %\n",
      "Accuracy of the network with dropout rate 0.5: 63.82 %\n",
      "Accuracy of plane with dropout rate 0.5: 74.0 %\n",
      "Accuracy of car with dropout rate 0.5: 77.5 %\n",
      "Accuracy of bird with dropout rate 0.5: 52.3 %\n",
      "Accuracy of cat with dropout rate 0.5: 35.0 %\n",
      "Accuracy of deer with dropout rate 0.5: 57.0 %\n",
      "Accuracy of dog with dropout rate 0.5: 59.9 %\n",
      "Accuracy of frog with dropout rate 0.5: 74.4 %\n",
      "Accuracy of horse with dropout rate 0.5: 69.2 %\n",
      "Accuracy of ship with dropout rate 0.5: 71.9 %\n",
      "Accuracy of truck with dropout rate 0.5: 67.0 %\n",
      "Accuracy of the network with dropout rate 0.6: 64.07 %\n",
      "Accuracy of plane with dropout rate 0.6: 74.9 %\n",
      "Accuracy of car with dropout rate 0.6: 78.0 %\n",
      "Accuracy of bird with dropout rate 0.6: 58.5 %\n",
      "Accuracy of cat with dropout rate 0.6: 45.0 %\n",
      "Accuracy of deer with dropout rate 0.6: 56.4 %\n",
      "Accuracy of dog with dropout rate 0.6: 45.0 %\n",
      "Accuracy of frog with dropout rate 0.6: 77.1 %\n",
      "Accuracy of horse with dropout rate 0.6: 62.6 %\n",
      "Accuracy of ship with dropout rate 0.6: 77.0 %\n",
      "Accuracy of truck with dropout rate 0.6: 66.2 %\n",
      "Accuracy of the network with dropout rate 0.7: 62.41 %\n",
      "Accuracy of plane with dropout rate 0.7: 62.5 %\n",
      "Accuracy of car with dropout rate 0.7: 80.0 %\n",
      "Accuracy of bird with dropout rate 0.7: 49.7 %\n",
      "Accuracy of cat with dropout rate 0.7: 31.7 %\n",
      "Accuracy of deer with dropout rate 0.7: 53.2 %\n",
      "Accuracy of dog with dropout rate 0.7: 61.4 %\n",
      "Accuracy of frog with dropout rate 0.7: 81.3 %\n",
      "Accuracy of horse with dropout rate 0.7: 58.9 %\n",
      "Accuracy of ship with dropout rate 0.7: 78.2 %\n",
      "Accuracy of truck with dropout rate 0.7: 67.2 %\n"
     ]
    }
   ],
   "source": [
    "dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)  # Placeholder dropout rate\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    " # List of dropout rates to try\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    model = ConvNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    model.dropout.p = dropout_rate  # Set the dropout rate\n",
    "\n",
    "    total_steps = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        n_class_correct = [0 for _ in range(10)]\n",
    "        n_class_samples = [0 for _ in range(10)]\n",
    "\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            n_samples += labels.size(0)\n",
    "            n_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                pred = predicted[i]\n",
    "\n",
    "                n_class_samples[label] += 1\n",
    "                if label == pred:\n",
    "                    n_class_correct[label] += 1\n",
    "\n",
    "        acc = 100.0 * n_correct / n_samples\n",
    "        print(f'Accuracy of the network with dropout rate {dropout_rate}: {acc} %')\n",
    "\n",
    "        for i in range(10):\n",
    "            acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "            print(f'Accuracy of {classes[i]} with dropout rate {dropout_rate}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fashion MNIST\n",
    "hep daha iyiye gitti acayip bir şekilde beklediğimizden de iyi (bence bir hata var ama salla\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "ml78hS4Yqtqm"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define batch size for training\n",
    "batch_size = 256\n",
    "\n",
    "# Transform the data to tensors and normalize it\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5,))])\n",
    "\n",
    "# Download and load the FashionMNIST dataset\n",
    "trainset = torchvision.datasets.FashionMNIST(root='datasets/', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='datasets/', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Define the classes in the FashionMNIST dataset\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
    "           'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top: 81.70%\n",
      "Accuracy of Trouser: 88.70%\n",
      "Accuracy of Pullover: 4.10%\n",
      "Accuracy of Dress: 13.40%\n",
      "Accuracy of Coat: 66.10%\n",
      "Accuracy of Sandal: 75.90%\n",
      "Accuracy of Shirt: 5.00%\n",
      "Accuracy of Sneaker: 81.20%\n",
      "Accuracy of Bag: 4.80%\n",
      "Accuracy of Ankle boot: 82.30%\n",
      "Overall accuracy on the 10000 test images: 50.32%\n"
     ]
    }
   ],
   "source": [
    "#architecture 1\n",
    "batch_size = 256\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the CNN\n",
    "net = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move the network to the GPU if available\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'Accuracy of {classes[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * correct / total\n",
    "print(f'Overall accuracy on the {len(testset)} test images: {overall_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top: 10.90%\n",
      "Accuracy of Trouser: 86.40%\n",
      "Accuracy of Pullover: 34.40%\n",
      "Accuracy of Dress: 55.40%\n",
      "Accuracy of Coat: 88.00%\n",
      "Accuracy of Sandal: 25.80%\n",
      "Accuracy of Shirt: 0.40%\n",
      "Accuracy of Sneaker: 84.10%\n",
      "Accuracy of Bag: 74.80%\n",
      "Accuracy of Ankle boot: 94.70%\n",
      "Overall accuracy on the 10000 test images: 55.49%\n"
     ]
    }
   ],
   "source": [
    "#architecture 2\n",
    "batch_size = 256\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the CNN\n",
    "net = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move the network to the GPU if available\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'Accuracy of {classes[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * correct / total\n",
    "print(f'Overall accuracy on the {len(testset)} test images: {overall_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top: 64.00%\n",
      "Accuracy of Trouser: 92.00%\n",
      "Accuracy of Pullover: 22.50%\n",
      "Accuracy of Dress: 55.20%\n",
      "Accuracy of Coat: 71.70%\n",
      "Accuracy of Sandal: 79.90%\n",
      "Accuracy of Shirt: 7.30%\n",
      "Accuracy of Sneaker: 78.10%\n",
      "Accuracy of Bag: 78.10%\n",
      "Accuracy of Ankle boot: 92.70%\n",
      "Overall accuracy on the 10000 test images: 64.15%\n"
     ]
    }
   ],
   "source": [
    "#architecture 3\n",
    "batch_size = 256\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the CNN\n",
    "net = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move the network to the GPU if available\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'Accuracy of {classes[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * correct / total\n",
    "print(f'Overall accuracy on the {len(testset)} test images: {overall_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of T-shirt/top: 82.10%\n",
      "Accuracy of Trouser: 94.60%\n",
      "Accuracy of Pullover: 71.50%\n",
      "Accuracy of Dress: 84.90%\n",
      "Accuracy of Coat: 85.30%\n",
      "Accuracy of Sandal: 91.50%\n",
      "Accuracy of Shirt: 37.80%\n",
      "Accuracy of Sneaker: 95.00%\n",
      "Accuracy of Bag: 94.80%\n",
      "Accuracy of Ankle boot: 92.10%\n",
      "Overall accuracy on the 10000 test images: 82.96%\n"
     ]
    }
   ],
   "source": [
    "#architecture 3 with momentum\n",
    "batch_size = 256\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Create an instance of the CNN\n",
    "net = CNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001,momentum=0.9)# Set the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Move the network to the GPU if available\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        class_correct = [0] * 10\n",
    "class_total = [0] * 10\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += (predicted[i] == label).item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(10):\n",
    "    accuracy = 100 * class_correct[i] / class_total[i]\n",
    "    print(f'Accuracy of {classes[i]}: {accuracy:.2f}%')\n",
    "\n",
    "# Calculate overall accuracy\n",
    "overall_accuracy = 100 * correct / total\n",
    "print(f'Overall accuracy on the {len(testset)} test images: {overall_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout rate: 0.2\n",
      "Accuracy with dropout rate 0.2: 81.50%\n",
      "Training with dropout rate: 0.3\n",
      "Accuracy with dropout rate 0.3: 85.92%\n",
      "Training with dropout rate: 0.4\n",
      "Accuracy with dropout rate 0.4: 87.61%\n",
      "Training with dropout rate: 0.5\n",
      "Accuracy with dropout rate 0.5: 87.96%\n",
      "Training with dropout rate: 0.6\n",
      "Accuracy with dropout rate 0.6: 88.97%\n",
      "Training with dropout rate: 0.7\n",
      "Accuracy with dropout rate 0.7: 89.10%\n"
     ]
    }
   ],
   "source": [
    "#architecture 3 with momentum and dropout at layer 2\n",
    "dropout_rates = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "batch_size = 256\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, dropout_rate):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Move the network to the GPU if available\n",
    "net = CNN(dropout_rates[0]).to(device)\n",
    "\n",
    "# Training and evaluation loop for different dropout rates\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"Training with dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    # Update the dropout rate in the network\n",
    "    net.dropout.p = dropout_rate\n",
    "    \n",
    "    # Reset optimizer and loss function for each dropout rate\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # Get the inputs and labels\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate accuracy for the current dropout rate\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    print(f'Accuracy with dropout rate {dropout_rate}: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout u nereye yerleştirdiğimiz çok fark ettiriyor bunu da ekleyebiliriz, tam olarak hangi layere eklediğimizi anlamadğım için şimdilik ellemiyorum, burada yazanlar iyi sonuç verenler sadece.\n",
    "artı olarak aynı değişken 2-3 farklı yerde tanımlanıyor kullanılmıyor falan daha temize çekilmesi lazım notebookun, ama test sonuçları kullanılabilir sonuçlar."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
